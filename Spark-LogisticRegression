from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.sql.functions import col
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import VectorAssembler,StringIndexer
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
spark = SparkSession.builder.appName("IrisClassification").getOrCreate()
iris = spark.read.csv("/user/maria_dev/assignment3/Iris.csv", inferSchema=True, header=True)
iris.dropna()
iris.show()
indexer = StringIndexer(inputCol="Species", outputCol="label")
iris = indexer.fit(iris).transform(iris)
iris.show()
assembler = VectorAssembler(inputCols=[col for col in iris.columns if col not in ['Id', 'Species', 'label']], outputCol="features")
iris = assembler.transform(iris)

train, test = iris.randomSplit([0.7, 0.3], seed=42)
train_count = train.count()
test_count = test.count()
print("Training Dataset Count: " + str(train_count))
print("Test Dataset Count: " + str(test_count))

lr = LogisticRegression(labelCol = "label",featuresCol = "features")

paramGrid = ParamGridBuilder() \
    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \
    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \
    .build()
crossval = CrossValidator(estimator=lr,
                          estimatorParamMaps=paramGrid,
                          evaluator=MulticlassClassificationEvaluator(labelCol="label",metricName="accuracy"),
                          numFolds=5)

cvModel = crossval.fit(train)
best_model = cvModel.bestModel
print("Best parameters: regParam: {}, elasticNetParam: {}".format(best_model._java_obj.getRegParam(), best_model._java_obj.getElasticNetParam()))
print("Best cross-validation score: {:.2f}".format(cvModel.avgMetrics[0]))

predictions = best_model.transform(test)

evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction")
accuracy = evaluator.evaluate(predictions,{evaluator.metricName:"accuracy"})
precision = evaluator.evaluate(predictions,{evaluator.metricName:"weightedPrecision"})
recall = evaluator.evaluate(predictions,{evaluator.metricName:"weightedRecall"})
f1 = evaluator.evaluate(predictions,{evaluator.metricName:"f1"})
print("Test set accuracy: {:.3f}".format(accuracy))
print("Test set precision: {:.3f}".format(precision))
print("Test set recall: {:.3f}".format(recall))
print("F1 score: {:.3f}".format(f1))

spark.stop()
